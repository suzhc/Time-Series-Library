{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.data_factory import data_provider\n",
    "from exp.exp_basic import Exp_Basic\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, cal_accuracy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models import Autoformer, Transformer, TimesNet, Nonstationary_Transformer, DLinear, FEDformer, \\\n",
    "    Informer, LightTS, Reformer, ETSformer, Pyraformer, PatchTST, MICN, Crossformer, FiLM\n",
    "from data_provider.uea import subsample, interpolate_missing, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg():\n",
    "    task_name = 'classification'\n",
    "    is_training = 1\n",
    "    model_id = 'test'\n",
    "    model = 'Autoformer'\n",
    "    data = 'ETTm1'\n",
    "    root_path = './data/ETT/'\n",
    "    data_path = 'ETTh1.csv'\n",
    "    features = 'M'\n",
    "    target = 'OT'\n",
    "    freq = 'h'\n",
    "    checkpoints = './checkpoints/'\n",
    "    seq_len = 96\n",
    "    label_len = 48\n",
    "    pred_len = 96\n",
    "    seasonal_patterns = 'Monthly'\n",
    "    mask_rate = 0.25\n",
    "    anomaly_ratio = 0.25\n",
    "    top_k = 5\n",
    "    num_kernels = 6\n",
    "    enc_in = 7\n",
    "    dec_in = 7\n",
    "    c_out = 7\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    e_layers = 2\n",
    "    d_layers = 1\n",
    "    d_ff = 2048\n",
    "    moving_avg = 25\n",
    "    factor = 1\n",
    "    distil = True\n",
    "    dropout = 0.1\n",
    "    embed = 'timeF'\n",
    "    activation = 'gelu'\n",
    "    output_attention = True\n",
    "    num_workers = 10\n",
    "    itr = 1\n",
    "    train_epochs = 10\n",
    "    batch_size = 32\n",
    "    patience = 3\n",
    "    learning_rate = 0.0001\n",
    "    des = 'test'\n",
    "    loss = 'MSE'\n",
    "    lradj = 'type1'\n",
    "    use_amp = False\n",
    "    use_gpu = True\n",
    "    gpu = 0\n",
    "    use_multi_gpu = False\n",
    "    devices = '0,1,2,3'\n",
    "    p_hidden_dims = [128, 128]\n",
    "    p_hidden_layers = 2\n",
    "\n",
    "args = arg()\n",
    "args.task_name = 'classification'\n",
    "args.is_training = 1\n",
    "args.root_path = './dataset/Huawei/'\n",
    "args.model_id = 'Huawei'\n",
    "args.model = 'TimesNet'\n",
    "args.data = 'Huawei'\n",
    "args.e_layers = 2\n",
    "args.batch_size = 16\n",
    "args.d_model = 64\n",
    "args.d_ff = 64\n",
    "args.top_k = 1\n",
    "args.des = 'Exp'\n",
    "args.itr = 1\n",
    "args.learning_rate = 0.001\n",
    "args.train_epochs = 30\n",
    "args.patience = 10\n",
    "args.use_gpu = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(flag):\n",
    "    data_set, data_loader = data_provider(args, flag)\n",
    "    return data_set, data_loader\n",
    "\n",
    "def select_optimizer():\n",
    "    model_optim = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    return model_optim\n",
    "\n",
    "def select_criterion():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion\n",
    "\n",
    "model_dict = {\n",
    "            'TimesNet': TimesNet,\n",
    "            'Autoformer': Autoformer,\n",
    "            'Transformer': Transformer,\n",
    "            'Nonstationary_Transformer': Nonstationary_Transformer,\n",
    "            'DLinear': DLinear,\n",
    "            'FEDformer': FEDformer,\n",
    "            'Informer': Informer,\n",
    "            'LightTS': LightTS,\n",
    "            'Reformer': Reformer,\n",
    "            'ETSformer': ETSformer,\n",
    "            'PatchTST': PatchTST,\n",
    "            'Pyraformer': Pyraformer,\n",
    "            'MICN': MICN,\n",
    "            'Crossformer': Crossformer,\n",
    "            'FiLM': FiLM,\n",
    "        }\n",
    "def build_model():\n",
    "    # model input depends on data\n",
    "    train_data, train_loader = get_data(flag='TRAIN')\n",
    "    test_data, test_loader = get_data(flag='TEST')\n",
    "    args.seq_len = max(train_data.max_seq_len, test_data.max_seq_len)\n",
    "    args.pred_len = 0\n",
    "    args.enc_in = train_data.feature_df.shape[1]\n",
    "    args.num_class = len(train_data.class_names)\n",
    "    # model init\n",
    "    model = model_dict[args.model].Model(args).float()\n",
    "    if args.use_multi_gpu and args.use_gpu:\n",
    "        model = nn.DataParallel(model, device_ids=args.device_ids)\n",
    "    return model\n",
    "\n",
    "def acquire_device():\n",
    "    if args.use_gpu:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\n",
    "            args.gpu) if not args.use_multi_gpu else args.devices\n",
    "        device = torch.device('cuda:{}'.format(args.gpu))\n",
    "        print('Use GPU: cuda:{}'.format(args.gpu))\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('Use CPU')\n",
    "    return device\n",
    "\n",
    "def load_data(filepath):\n",
    "    df = pd.read_pickle(filepath)\n",
    "    df = df.reset_index()\n",
    "    df['label'] = df['抑郁得分'].apply(lambda x: 'depression' if x > 4 else 'non-depression')\n",
    "    labels = df['label'].values\n",
    "    df = df[['heartrate', 'bloodoxygen', 'step']]\n",
    "    labels = pd.Series(labels, dtype=\"category\")\n",
    "    class_names = labels.cat.categories\n",
    "    labels_df = pd.DataFrame(labels.cat.codes,\n",
    "                                dtype=np.int8)  # int8-32 gives an error when using nn.CrossEntropyLoss\n",
    "\n",
    "    lengths = df.applymap(\n",
    "        lambda x: len(x)).values  # (num_samples, num_dimensions) array containing the length of each series\n",
    "\n",
    "    horiz_diffs = np.abs(lengths - np.expand_dims(lengths[:, 0], -1))\n",
    "\n",
    "    if np.sum(horiz_diffs) > 0:  # if any row (sample) has varying length across dimensions\n",
    "        df = df.applymap(subsample)\n",
    "\n",
    "    lengths = df.applymap(lambda x: len(x)).values\n",
    "    vert_diffs = np.abs(lengths - np.expand_dims(lengths[0, :], 0))\n",
    "    if np.sum(vert_diffs) > 0:  # if any column (dimension) has varying length across samples\n",
    "        max_seq_len = int(np.max(lengths[:, 0]))\n",
    "    else:\n",
    "        max_seq_len = lengths[0, 0]\n",
    "\n",
    "    # First create a (seq_len, feat_dim) dataframe for each sample, indexed by a single integer (\"ID\" of the sample)\n",
    "    # Then concatenate into a (num_samples * seq_len, feat_dim) dataframe, with multiple rows corresponding to the\n",
    "    # sample index (i.e. the same scheme as all datasets in this project)\n",
    "\n",
    "    # df = pd.concat((pd.DataFrame({col: df.loc[row, col] for col in df.columns}).reset_index(drop=True).set_index(\n",
    "    #     pd.Series(lengths[row, 0] * [row])) for row in range(df.shape[0])), axis=0)\n",
    "    \n",
    "    a_list = []\n",
    "\n",
    "    for row in range(df.shape[0]):\n",
    "        a = pd.DataFrame({col: df.loc[row, col] for col in df.columns}).reset_index(drop=True)\n",
    "        a = a.set_index(pd.Series(lengths[row, 0] * [row]))\n",
    "        a_list.append(a)\n",
    "\n",
    "    df = pd.concat(a_list, axis=0)\n",
    "\n",
    "    # Replace NaN values\n",
    "    grp = df.groupby(by=df.index)\n",
    "    df = grp.transform(interpolate_missing)\n",
    "\n",
    "    return df, labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "11\n",
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "    args.task_name,\n",
    "    args.model_id,\n",
    "    args.model,\n",
    "    args.data,\n",
    "    args.features,\n",
    "    args.seq_len,\n",
    "    args.label_len,\n",
    "    args.pred_len,\n",
    "    args.d_model,\n",
    "    args.n_heads,\n",
    "    args.e_layers,\n",
    "    args.d_layers,\n",
    "    args.d_ff,\n",
    "    args.factor,\n",
    "    args.embed,\n",
    "    args.distil,\n",
    "    args.des, 0)\n",
    "model = build_model()\n",
    "device = acquire_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(vali_data, vali_loader, criterion):\n",
    "    total_loss = []\n",
    "    preds = []\n",
    "    trues = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, label, padding_mask) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            padding_mask = padding_mask.float().to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            outputs = model(batch_x, padding_mask, None, None)\n",
    "\n",
    "            pred = outputs.detach().cpu()\n",
    "            loss = criterion(pred, label.long().squeeze().cpu())\n",
    "            total_loss.append(loss)\n",
    "\n",
    "            preds.append(outputs.detach())\n",
    "            trues.append(label)\n",
    "\n",
    "    total_loss = np.average(total_loss)\n",
    "\n",
    "    preds = torch.cat(preds, 0)\n",
    "    trues = torch.cat(trues, 0)\n",
    "    probs = torch.nn.functional.softmax(preds)  # (total_samples, num_classes) est. prob. for each class and sample\n",
    "    predictions = torch.argmax(probs, dim=1).cpu().numpy()  # (total_samples,) int class index for each sample\n",
    "    trues = trues.flatten().cpu().numpy()\n",
    "    accuracy = cal_accuracy(predictions, trues)\n",
    "\n",
    "    model.train()\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_data, train_loader = get_data(flag='TRAIN')\n",
    "    vali_data, vali_loader = get_data(flag='TRAIN')\n",
    "    test_data, test_loader = get_data(flag='TRAIN')\n",
    "\n",
    "    path = os.path.join(args.checkpoints, setting)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    time_now = time.time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "\n",
    "    model_optim = select_optimizer()\n",
    "    criterion = select_criterion()\n",
    "\n",
    "    for epoch in range(args.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "\n",
    "        for i, (batch_x, label, padding_mask) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            padding_mask = padding_mask.float().to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            outputs = model(batch_x, padding_mask, None, None)\n",
    "            loss = criterion(outputs, label.long().squeeze(-1))\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time.time() - time_now) / iter_count\n",
    "                left_time = speed * ((args.train_epochs - epoch) * train_steps - i)\n",
    "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time.time()\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "            model_optim.step()\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss, val_accuracy = vali(vali_data, vali_loader, criterion)\n",
    "        test_loss, test_accuracy = vali(test_data, test_loader, criterion)\n",
    "\n",
    "        print(\n",
    "            \"Epoch: {0}, Steps: {1} | Train Loss: {2:.3f} Vali Loss: {3:.3f} Vali Acc: {4:.3f} Test Loss: {5:.3f} Test Acc: {6:.3f}\"\n",
    "            .format(epoch + 1, train_steps, train_loss, vali_loss, val_accuracy, test_loss, test_accuracy))\n",
    "        early_stopping(-val_accuracy, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            adjust_learning_rate(model_optim, epoch + 1, args)\n",
    "def test():\n",
    "    test_data, test_loader = get_data(flag='TRAIN')\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    folder_path = './test_results/' + setting + '/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, label, padding_mask) in enumerate(test_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            padding_mask = padding_mask.float().to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            outputs = model(batch_x, padding_mask, None, None)\n",
    "\n",
    "            preds.append(outputs.detach())\n",
    "            trues.append(label)\n",
    "\n",
    "    preds = torch.cat(preds, 0)\n",
    "    trues = torch.cat(trues, 0)\n",
    "    print('test shape:', preds.shape, trues.shape)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(preds)  # (total_samples, num_classes) est. prob. for each class and sample\n",
    "    predictions = torch.argmax(probs, dim=1).cpu().numpy()  # (total_samples,) int class index for each sample\n",
    "    trues = trues.flatten().cpu().numpy()\n",
    "    accuracy = cal_accuracy(predictions, trues)\n",
    "    print('test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n",
      "Epoch: 1 cost time: 0.7035861015319824\n",
      "Epoch: 1, Steps: 3 | Train Loss: 1.935 Vali Loss: 0.695 Vali Acc: 0.750 Test Loss: 0.630 Test Acc: 0.725\n",
      "Validation loss decreased (inf --> -0.750000).  Saving model ...\n",
      "Epoch: 2 cost time: 0.6311109066009521\n",
      "Epoch: 2, Steps: 3 | Train Loss: 0.522 Vali Loss: 0.254 Vali Acc: 0.850 Test Loss: 0.249 Test Acc: 0.875\n",
      "Validation loss decreased (-0.750000 --> -0.850000).  Saving model ...\n",
      "Epoch: 3 cost time: 0.7240602970123291\n",
      "Epoch: 3, Steps: 3 | Train Loss: 0.154 Vali Loss: 0.100 Vali Acc: 0.975 Test Loss: 0.114 Test Acc: 0.975\n",
      "Validation loss decreased (-0.850000 --> -0.975000).  Saving model ...\n",
      "Epoch: 4 cost time: 0.7057321071624756\n",
      "Epoch: 4, Steps: 3 | Train Loss: 0.066 Vali Loss: 0.003 Vali Acc: 1.000 Test Loss: 0.019 Test Acc: 0.975\n",
      "Validation loss decreased (-0.975000 --> -1.000000).  Saving model ...\n",
      "Epoch: 5 cost time: 0.6929035186767578\n",
      "Epoch: 5, Steps: 3 | Train Loss: 0.007 Vali Loss: 0.106 Vali Acc: 0.950 Test Loss: 0.417 Test Acc: 0.900\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.25e-05\n",
      "Epoch: 6 cost time: 0.5807864665985107\n",
      "Epoch: 6, Steps: 3 | Train Loss: 0.115 Vali Loss: 0.063 Vali Acc: 0.950 Test Loss: 0.099 Test Acc: 0.950\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: 7 cost time: 0.8001554012298584\n",
      "Epoch: 7, Steps: 3 | Train Loss: 0.538 Vali Loss: 0.020 Vali Acc: 1.000 Test Loss: 0.559 Test Acc: 0.875\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 8 cost time: 0.5464932918548584\n",
      "Epoch: 8, Steps: 3 | Train Loss: 0.017 Vali Loss: 0.004 Vali Acc: 1.000 Test Loss: 0.004 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 9 cost time: 0.6420409679412842\n",
      "Epoch: 9, Steps: 3 | Train Loss: 0.005 Vali Loss: 0.001 Vali Acc: 1.000 Test Loss: 0.002 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 10 cost time: 0.6897008419036865\n",
      "Epoch: 10, Steps: 3 | Train Loss: 0.002 Vali Loss: 0.003 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "Epoch: 11 cost time: 0.7851760387420654\n",
      "Epoch: 11, Steps: 3 | Train Loss: 0.004 Vali Loss: 0.001 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 12 cost time: 0.6067686080932617\n",
      "Epoch: 12, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.000 Vali Acc: 1.000 Test Loss: 0.001 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 13 cost time: 0.7370834350585938\n",
      "Epoch: 13, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.001 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 14 cost time: 0.7450606822967529\n",
      "Epoch: 14, Steps: 3 | Train Loss: 0.002 Vali Loss: 0.000 Vali Acc: 1.000 Test Loss: 0.001 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 15 cost time: 0.8485391139984131\n",
      "Epoch: 15, Steps: 3 | Train Loss: 0.004 Vali Loss: 0.001 Vali Acc: 1.000 Test Loss: 0.005 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-08\n",
      "Epoch: 16 cost time: 0.7713418006896973\n",
      "Epoch: 16, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.000 Vali Acc: 1.000 Test Loss: 0.001 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 17 cost time: 0.578192949295044\n",
      "Epoch: 17, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.003 Vali Acc: 1.000 Test Loss: 0.001 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 18 cost time: 0.7237317562103271\n",
      "Epoch: 18, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.000 Vali Acc: 1.000 Test Loss: 0.001 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 19 cost time: 0.6993575096130371\n",
      "Epoch: 19, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.008 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 20 cost time: 0.625450611114502\n",
      "Epoch: 20, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.000 Vali Acc: 1.000 Test Loss: 0.013 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-09\n",
      "Epoch: 21 cost time: 0.7840702533721924\n",
      "Epoch: 21, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.003 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 22 cost time: 0.5535686016082764\n",
      "Epoch: 22, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.001 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 23 cost time: 0.8379342555999756\n",
      "Epoch: 23, Steps: 3 | Train Loss: 0.006 Vali Loss: 0.003 Vali Acc: 1.000 Test Loss: 0.001 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 24 cost time: 0.7521216869354248\n",
      "Epoch: 24, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.000 Vali Acc: 1.000 Test Loss: 0.004 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 25 cost time: 0.7049181461334229\n",
      "Epoch: 25, Steps: 3 | Train Loss: 0.004 Vali Loss: 0.001 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-11\n",
      "Epoch: 26 cost time: 0.6733360290527344\n",
      "Epoch: 26, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.001 Vali Acc: 1.000 Test Loss: 0.001 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 27 cost time: 0.5907959938049316\n",
      "Epoch: 27, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.011 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 28 cost time: 0.6794350147247314\n",
      "Epoch: 28, Steps: 3 | Train Loss: 0.008 Vali Loss: 0.000 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 29 cost time: 0.7834136486053467\n",
      "Epoch: 29, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.015 Vali Acc: 1.000 Test Loss: 0.000 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Epoch: 30 cost time: 0.649512767791748\n",
      "Epoch: 30, Steps: 3 | Train Loss: 0.001 Vali Loss: 0.000 Vali Acc: 1.000 Test Loss: 0.001 Test Acc: 1.000\n",
      "Validation loss decreased (-1.000000 --> -1.000000).  Saving model ...\n",
      "Updating learning rate to 1.862645149230957e-12\n",
      "40\n",
      "test shape: torch.Size([40, 2]) torch.Size([40, 1])\n",
      "test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "train()\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
